{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: read and write .csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wherever you have the current file where you work on this problem, create ONE LEVEL UP a directory called \"data\" and put there the datafile [`amazon_stock_data.csv`](https://raw.githubusercontent.com/paljenczy/progtools2-2016-winter/master/data/amazon_stock_data.csv). This data contains data on Amazon's stock price on a daily frequency.\n",
    "\n",
    "1. Read the data using the `csv` module's `DictReader` function and build a list of the daily price data. That is, this will be a list of dictionaries where each dictionary contains data from a day.\n",
    "\n",
    "2. Go through this list and for each dictionary, create a new key-value pair: `\"Avg\"` will be the key and the value will be the average of the `Open` and `Close` prices.\n",
    "    * define an empty list `list_new_dicts` in which you'll collect the new dictionaries\n",
    "    * write a for loop that steps thourgh the dictionaries\n",
    "        * from each dictionary, get the value corresponding to the keys `\"Open\"` and `\"Close\"`. These are strings, turn them into `float`s using the `float` function\n",
    "        * take the average of these two float numbers and assign it to a variable `avg_price`. Its type is `float`.\n",
    "        * add the new key-value pair `\"Avg\"` as key and the calculated average as value. CAUTION! Turn fist the float value into string using `\"{%.2f}\".format(avg_price)`\n",
    "\n",
    "3. Take the new list and write it using `csv.DictWriter`. You have to supply an argument `fieldnames` that specifies the fields that are written (just copy and paste, and inspect how you can use it).\n",
    "\n",
    "```\n",
    "with open(\"../data/amazon_stock_data.csv\", \"w\") as f:\n",
    "      writer = csv.DictWriter(f, fieldnames=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\",\"Avg\"],\n",
    "                              delimiter=\",\")\n",
    "      # write a header\n",
    "      writer.writeheader()\n",
    "      # loop through the new list of dictionaries\n",
    "      for d in list_new_dicts:\n",
    "            writer.writerow(d)\n",
    "      \n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.\n",
    "import csv\n",
    "\n",
    "list_dicts = []\n",
    "with open(\"../data/amazon_stock_data.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\",\")\n",
    "    for d in reader:\n",
    "        list_dicts.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.\n",
    "list_new_dicts = []\n",
    "for d in list_dicts:\n",
    "    avg_price = (float(d[\"Open\"]) + float(d[\"Close\"]))/2\n",
    "    d[\"Avg\"] = \"{:.2f}\".format(avg_price)\n",
    "    list_new_dicts.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.\n",
    "with open(\"../data/amazon_stock_data_new.csv\", \"w\") as f:\n",
    "      writer = csv.DictWriter(f, fieldnames=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\",\"Avg\"],\n",
    "                              delimiter=\",\")\n",
    "      # write a header\n",
    "      writer.writeheader()\n",
    "      # loop through the new list of dictionaries\n",
    "      for d in list_new_dicts:\n",
    "            writer.writerow(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Mining a news article with BS and regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the news article http://www.sacbee.com/site-services/databases/article57154063.html. \n",
    "\n",
    "1. Extract the title of the article.\n",
    "2. Extract the author information to a string variable (BY PHILLIP REESE - PREESE@SACBEE.COM)\n",
    "3. Write a function that extracts an email address from a text\n",
    "  * create a regular expression that parses the email address and returns that\n",
    "  * search if the regex matches the input string\n",
    "  * if yes, return the email address, if not, return `None`\n",
    "    * you can test if a match was found using\n",
    "      ```\n",
    "      result = pattern.search(text)\n",
    "      if bool(result):\n",
    "        ...\n",
    "      ```\n",
    "4. Extract the date and time information appearing above the title (JANUARY 28, 2016 4:26 PM)\n",
    "5. Write a function that extracts time information from a string\n",
    "  * it should match \"hour:minute AM/PM\"-like strings\n",
    "    * hint: `[0-5]` matches all digits between 0 and 5\n",
    "    * 2:33 PM, 22:40 AM should be matched\n",
    "    * times that does not make sense should not be matched (like 45:33 PM or 2:67 AM)\n",
    "    * return the time if found and `None` otherwise\n",
    "  * use this function to extract the time from the string extracted in the previous task\n",
    "6. The text contains __two__ links to other news articles. Extract them using a combination of regex with Beautiful Soup search.\n",
    "  * the links should appear in the body of the text, not in other linked content (thus, first you should extract a BS object that is the body of the text\n",
    "  * the links are contained in `<a>` tags' `href` attributes\n",
    "  * the links should match a regex that begins with `http` and end in `html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# set the default xml reader:\n",
    "\n",
    "def BS(html):\n",
    "    \"\"\"\n",
    "    Overcome annoying warning message: set the html parser\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.sacbee.com/site-services/databases/article57154063.html\"\n",
    "soup = BS(urlopen(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top political parties continue to slip in Sacramento region\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# this is a Beautiful Soup object\n",
    "title_obj = soup.find(\"h1\", {\"class\": \"title\"})\n",
    "\n",
    "# get the text and strip newline characters\n",
    "print(title_obj.get_text().replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Phillip Reese - preese@sacbee.com\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "author_text = soup.find(\"div\", {\"class\": \"byline element-spacing-small\"}).p.get_text()\n",
    "print(author_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preese@sacbee.com\n"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "\n",
    "import re\n",
    "\n",
    "def email_extractor(text):\n",
    "    email_pattern = re.compile(\"\\w+@\\w+\\.\\w+\")\n",
    "    result = email_pattern.search(text)\n",
    "    if bool(result):\n",
    "        email_address = result.group()\n",
    "        return email_address\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "email_address = email_extractor(author_text)\n",
    "print(email_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "January 28, 2016 4:26 PM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.\n",
    "time_and_date = soup.find(\"p\", {\"class\": \"published-date\"}).get_text()\n",
    "print(time_and_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:26 PM\n"
     ]
    }
   ],
   "source": [
    "# 5.\n",
    "\n",
    "def extract_time(text):\n",
    "    time_pattern = re.compile(\"[1-2]?[0-9]:[0-5][0-9] (A|P)M\")\n",
    "    match = time_pattern.search(text)\n",
    "    if bool(match):\n",
    "        return match.group()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(extract_time(time_and_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.aipca.org/platform.html\">platform planks</a>, <a href=\"http://www.sacbee.com/site-services/databases/article4457401.html\">he political makeup of every Sacramento neighborhood</a>]\n",
      "['http://www.aipca.org/platform.html', 'http://www.sacbee.com/site-services/databases/article4457401.html']\n"
     ]
    }
   ],
   "source": [
    "# 6.\n",
    "story = soup.find(\"div\", {\"id\": \"story-body-items\"})\n",
    "links_in_story = story.findAll(\"a\", {\"href\": re.compile(\"^http.*html$\")})\n",
    "print(links_in_story)\n",
    "\n",
    "# extract the links\n",
    "list_links_in_text = [x[\"href\"] for x in links_in_story]\n",
    "print(list_links_in_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
